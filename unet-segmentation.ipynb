{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNET SEGMENTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arxiv Link: <a href=\"https://arxiv.org/abs/1505.04597\">U-Net: Convolutional Networks for Biomedical Image Segmentation</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>UNet is a fully convolutional network(FCN) that does image segmentation. Its goal is to predict each pixel's class.</li>\n",
    " \n",
    "<li>UNet is built upon the FCN and modified in a way that it yields better segmentation in medical imaging.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Architecture\n",
    "\n",
    "<img src=\"images/u-net-architecture.png\"/>\n",
    "\n",
    "<h3>UNet Architecture has 3 parts:</h3>\n",
    "<ol>\n",
    "    <li>The Contracting/Downsampling Path</li>\n",
    "    <li>Bottleneck</li>\n",
    "    <li>The Expanding/Upsampling Path</li>\n",
    "</ol>\n",
    "\n",
    "<h3>Downsampling Path: </h3> \n",
    "<ol>\n",
    "    <li>It consists of two 3x3 convolutions (unpadded convolutions), each followed by a rectified linear unit (ReLU) and a 2x2 max pooling operation with stride 2 for downsampling.</li> \n",
    "    <li>At each downsampling step we double the number of feature channels.</li>\n",
    "</ol>\n",
    "\n",
    "<h3>Upsampling Path: </h3> \n",
    "<ol>\n",
    "     <li> Every  step  in  the  expansive  path  consists  of  an  upsampling  of  the feature map followed by a 2x2 convolution (“up-convolution”), a concatenation with the correspondingly feature  map  from  the  downsampling  path,  and  two  3x3  convolutions,  each  followed by a ReLU.</li>\n",
    "</ol>\n",
    "\n",
    "<h3> Skip Connection: </h3>\n",
    "The skip connection from the downsampling path are concatenated with feature map during upsampling path. These skip connection provide local information to global information while upsampling.\n",
    "\n",
    "<h3> Final Layer: </h3>\n",
    "At the final layer a 1x1 convolution is used to map each feature vector to the desired number of classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Advantages\n",
    "<h3> Advantages: </h3>\n",
    "<ol>\n",
    "    <li>The UNet combines the location information from the downsampling path to finally obtain a general information combining localisation and context, which is necessary to predict a good segmentation map.</li>\n",
    "    <li>No Dense layer is used, so image sizes can be used.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Dataset\n",
    "Link: <a href=\"https://www.kaggle.com/c/data-science-bowl-2018\">Data Science Bowl 2018</a>\n",
    "Find the nuclei in divergent images to advance medical discovery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n## Seeding \\nseed = 2019\\nrandom.seed = seed\\nnp.random.seed = seed\\ntf.seed = seed'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Imports\n",
    "import os\n",
    "import os.path\n",
    "import sys\n",
    "import random\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.models import * \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from data_prep import *\n",
    "\n",
    "#Tensorboard for Visualization\n",
    "from keras.callbacks import TensorBoard \n",
    "import time\n",
    "\n",
    "\"\"\"\n",
    "## Seeding \n",
    "seed = 2019\n",
    "random.seed = seed\n",
    "np.random.seed = seed\n",
    "tf.seed = seed\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend.tensorflow_backend as K\n",
    "\n",
    "K.set_session\n",
    "import tensorflow as tf\n",
    "#K.tensorflow_backend._get_available_gpus()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Config File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import json\n",
    "with open('/Users/vasudevsharma/Desktop/axondeepseg-master-2/AxonDeepSeg/models/default_SEM_model_v1/config_network.json') as djson:\n",
    "    training_config = json.load(djson)\n",
    "print(training_config) \"\"\"\n",
    "# Example of network configuration for TEM data (small network trainable on a Titan X GPU card)\n",
    "training_config = {\n",
    "    \n",
    "# General parameters:    \n",
    "  \"n_classes\": 3,  # Number of classes. For this application, the number of classes should be set to **3** (i.e. axon pixel, myelin pixel, or background pixel).\n",
    "  \"thresholds\": [0, 0.2, 0.8],  # Thresholds for the 3-class classification problem. Do not modify.  \n",
    "  \"trainingset_patchsize\": 512,  # Patch size of the training set in pixels (note that the patches have the same size in both dimensions).  \n",
    "  \"trainingset\": \"TEM_3c_512\",  # Name of the training set.\n",
    "  \"batch_size\": 8,  # Batch size, i.e. the number of training patches used in one iteration of the training. Note that a larger batch size will take more memory.\n",
    "\n",
    "# Network architecture parameters:     \n",
    "  \"depth\": 4,  # Depth of the network (i.e. number of blocks of the U-net).\n",
    "  \"convolution_per_layer\": [2, 2, 2, 2],  # Number of convolution layers used at each block.\n",
    "  \"size_of_convolutions_per_layer\": [[5, 5], [3, 3], [3, 3], [3, 3]],  # Kernel size of each convolution layer of the network.\n",
    "  \"features_per_convolution\": [[[1, 16], [16, 16]], [[16, 32], [32, 32]], [[32, 64], [64, 64]], [[64, 128], [128, 128]]],  # Number of features of each convolution layer.\n",
    "  \"downsampling\": \"convolution\",  # Type of downsampling to use in the downsampling layers of the network. Option \"maxpooling\" for standard max pooling layer or option \"convolution\" for learned convolutional downsampling.\n",
    "  \"dropout\": 0.75,  # Dropout to use for the training. Note: In TensorFlow, the keep probability is used instead. For instance, setting this param. to 0.75 means that 75% of the neurons of the network will be kept (i.e. dropout of 25%).\n",
    "     \n",
    "# Learning rate parameters:    \n",
    "  \"learning_rate\": 0.01,  # Learning rate to use in the training.  \n",
    "  \"learning_rate_decay_activate\": True,  # Set to \"True\" to use a decay on the learning rate.  \n",
    "  \"learning_rate_decay_period\": 24000,  # Period of the learning rate decay, expressed in number of images (samples) seen.\n",
    "  \"learning_rate_decay_type\": \"polynomial\",  # Type of decay to use. An exponential decay will be used by default unless this param. is set to \"polynomial\" (to use a polynomial decay).\n",
    "  \"learning_rate_decay_rate\": 0.99,  # Rate of the decay to use for the exponential decay. This only applies when the user does not set the decay type to \"polynomial\".\n",
    "    \n",
    "# Batch normalization parameters:     \n",
    "  \"batch_norm_activate\": True,  # Set to \"True\" to use batch normalization during the training.\n",
    "  \"batch_norm_decay_decay_activate\": True,  # Set to \"True\" to activate an exponential decay for the batch normalization step of the training.  \n",
    "  \"batch_norm_decay_starting_decay\": 0.7,  # The starting decay value for the batch normalization. \n",
    "  \"batch_norm_decay_ending_decay\": 0.9,  # The ending decay value for the batch normalization.\n",
    "  \"batch_norm_decay_decay_period\": 16000,  # Period of the batch normalization decay, expressed in number of images (samples) seen.\n",
    "        \n",
    "# Weighted cost parameters:    \n",
    "  \"weighted_cost-activate\": True,  # Set to \"True\" to use weights based on the class in the cost function for the training.\n",
    "  \"weighted_cost-balanced_activate\": True,  # Set to \"True\" to use weights in the cost function to correct class imbalance. \n",
    "  \"weighted_cost-balanced_weights\": [1.1, 1, 1.3],  # Values of the weights for the class imbalance. Typically, larger weights are assigned to classes with less pixels to add more penalty in the cost function when there is a misclassification. Order of the classes in the weights list: background, myelin, axon.\n",
    "  \"weighted_cost-boundaries_sigma\": 2,  # Set to \"True\" to add weights to the boundaries (e.g. penalize more when misclassification happens in the axon-myelin interface).\n",
    "  \"weighted_cost-boundaries_activate\": False,  # Value to control the distribution of the boundary weights (if activated). \n",
    "    \n",
    "# Data augmentation parameters:\n",
    "  \"da-type\": \"all\",  # Type of data augmentation procedure. Option \"all\" applies all selected data augmentation transformations sequentially, while option \"random\" only applies one of the selected transformations (randomly) to the sample(s). List of available data augmentation transformations: 'random_rotation', 'noise_addition', 'elastic', 'shifting', 'rescaling' and 'flipping'. \n",
    "  \"da-0-shifting-activate\": True, \n",
    "  \"da-1-rescaling-activate\": False,\n",
    "  \"da-2-random_rotation-activate\": False,  \n",
    "  \"da-3-elastic-activate\": True, \n",
    "  \"da-4-flipping-activate\": True, \n",
    "  \"da-5-noise_addition-activate\": False\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Convolutional Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def upconv(x, n_out_chan, scope, \\n              w_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\\n              training_phase=True, activate_bn = True, bn_decay = 0.999):\\n   \\n    \\n    with tf.variable_scope(scope):\\n        if activate_bn == True:\\n            net = tf.contrib.layers.conv2d(x, num_outputs=n_out_chan, kernel_size=3, stride=1, \\n                                       activation_fn=tf.nn.relu, normalizer_fn=tf.contrib.layers.batch_norm,\\n                                       normalizer_params={'scale':True, 'is_training':training_phase,\\n                                                          'decay':bn_decay, 'scope':'bn'},\\n                                       weights_initializer = w_initializer, scope='convolution'\\n                                      )\\n        else:\\n            net = tf.contrib.layers.conv2d(x, num_outputs=n_out_chan, kernel_size=3, stride=1, \\n                                       activation_fn=tf.nn.relu, weights_initializer = w_initializer, scope='convolution'\\n                                      )\\n        \\n        tf.add_to_collection('activations',net)\\n        return net\\n\\n\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def conv_relu(x, filters , kernel_size , strides , activation = 'relu', kernel_initializer = 'glorot_normal', activate_bn = True, bn_decay = 0.999, keep_prob=1.0):\n",
    "    if activate_bn == True:\n",
    "\n",
    "            net = Conv2D(filters = filters, kernel_size = kernel_size, strides = strides, padding = 'same', activation = activation, kernel_initializer = kernel_initializer)(x)\n",
    "            net = BatchNormalization(axis = 3, momentum = 1 - bn_decay)(net)\n",
    "            \n",
    "    else: \n",
    "            net = Conv2D(filters = filters, kernel_size = kernel_size, strides = strides, activation = activation, kernel_initializer = kernel_initializer, padding = 'same')(net)\n",
    "            \n",
    "    net =  Dropout(rate = 1 - keep_prob)(net)\n",
    "            \n",
    "    return net \n",
    "\n",
    "   \n",
    "def downconv(x, filters , kernel_size = 5, strides = 2, activation = 'relu', kernel_initializer = 'glorot_normal', activate_bn = True, bn_decay = 0.999):\n",
    "    if activate_bn == True:\n",
    "\n",
    "            net = Conv2D(filters = filters, kernel_size = kernel_size, strides = strides, padding = 'same',activation = activation, kernel_initializer = kernel_initializer)(x)\n",
    "            net = BatchNormalization(axis = 3, momentum = 1 - bn_decay)(net)\n",
    "\n",
    "    else:\n",
    "\n",
    "            net = Conv2D(filters = filters, kernel_size = kernel_size, strides = strides, activation = activation, kernel_initializer = kernel_initializer, padding = 'same')(net)\n",
    "            \n",
    "    return net\n",
    "\n",
    "\"\"\"def upconv(x, n_out_chan, scope, \n",
    "              w_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "              training_phase=True, activate_bn = True, bn_decay = 0.999):\n",
    "   \n",
    "    \n",
    "    with tf.variable_scope(scope):\n",
    "        if activate_bn == True:\n",
    "            net = tf.contrib.layers.conv2d(x, num_outputs=n_out_chan, kernel_size=3, stride=1, \n",
    "                                       activation_fn=tf.nn.relu, normalizer_fn=tf.contrib.layers.batch_norm,\n",
    "                                       normalizer_params={'scale':True, 'is_training':training_phase,\n",
    "                                                          'decay':bn_decay, 'scope':'bn'},\n",
    "                                       weights_initializer = w_initializer, scope='convolution'\n",
    "                                      )\n",
    "        else:\n",
    "            net = tf.contrib.layers.conv2d(x, num_outputs=n_out_chan, kernel_size=3, stride=1, \n",
    "                                       activation_fn=tf.nn.relu, weights_initializer = w_initializer, scope='convolution'\n",
    "                                      )\n",
    "        \n",
    "        tf.add_to_collection('activations',net)\n",
    "        return net\n",
    "\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ------------------------ NETWORK STRUCTURE ------------------------ #\n",
    "\n",
    "\n",
    "def uconv_net(training_config,bn_updated_decay = None, verbose = True):\n",
    "    \"\"\"\n",
    "    Create the U-net.\n",
    "    Input :\n",
    "        x : TF object to define, ensemble des patchs des images :graph input\n",
    "        config : dict : described in the header.\n",
    "        image_size : int : The image size\n",
    "\n",
    "    Output :\n",
    "        The U-net.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the variables\n",
    "    image_size = training_config[\"trainingset_patchsize\"]\n",
    "    n_classes = training_config[\"n_classes\"]\n",
    "    depth = training_config[\"depth\"]\n",
    "    dropout = training_config[\"dropout\"]\n",
    "    number_of_convolutions_per_layer = training_config[\"convolution_per_layer\"]\n",
    "    size_of_convolutions_per_layer = training_config[\"size_of_convolutions_per_layer\"]\n",
    "    features_per_convolution = training_config[\"features_per_convolution\"]\n",
    "    downsampling = training_config[\"downsampling\"]\n",
    "    activate_bn = training_config[\"batch_norm_activate\"]\n",
    "    if bn_updated_decay is None:\n",
    "        bn_decay = training_config[\"batch_norm_decay_starting_decay\"]\n",
    "    else:\n",
    "        bn_decay = bn_updated_decay\n",
    "\n",
    "    # Input picture shape is [batch_size, height, width, number_channels_in] (number_channels_in = 1 for the input layer)\n",
    "    \n",
    "    data_temp_size = [image_size]\n",
    "    relu_results = []\n",
    "\n",
    "    ####################################################################\n",
    "    ######################### CONTRACTION PHASE ########################\n",
    "    ####################################################################\n",
    "    \n",
    "\n",
    "\n",
    "   # print(data_temp)\n",
    "\n",
    "    #X = Input((image_size*image_size, 3))\n",
    "    X = Input((image_size, image_size, 3))\n",
    " \n",
    "    net = X\n",
    "    data_temp = X\n",
    "   # print(net.shape)\n",
    "    #print(depth, number_of_convolutions_per_layer)\n",
    "    for i in range(depth):\n",
    "\n",
    "        for conv_number in range(number_of_convolutions_per_layer[i]):\n",
    "            \n",
    "            if verbose:\n",
    "                #print(('Layer: ', i, ' Conv: ', conv_number, 'Features: ', features_per_convolution[i][conv_number]))\n",
    "                #print(('Size:', size_of_convolutions_per_layer[i][conv_number]))\n",
    "                \n",
    "                net = conv_relu(net, filters = features_per_convolution[i][conv_number][1], kernel_size = size_of_convolutions_per_layer[i][conv_number], strides = 1 , activation = 'relu', kernel_initializer = 'glorot_normal', activate_bn = True, bn_decay = 0.999, keep_prob=1.0)\n",
    "               \n",
    "        relu_results.append(net) # We keep them for the upconvolutions\n",
    "    \n",
    "\n",
    "        if downsampling == 'convolution':\n",
    "              \n",
    "            net = downconv(net, filters = features_per_convolution[i][conv_number][1], kernel_size = 5, strides = 2, activation = 'relu', kernel_initializer = 'glorot_normal', activate_bn = True, bn_decay = 0.999)\n",
    "      \n",
    "        else: \n",
    "\n",
    "            net = MaxPooling2D((2, 2), padding = 'valid', strides = 2,  name ='downmp-d'+str(i))(net)\n",
    "\n",
    "        data_temp_size.append(data_temp_size[-1] // 2)\n",
    "        data_temp = net\n",
    "             \n",
    "\n",
    "\n",
    "    ####################################################################\n",
    "    ########################## EXPANSION PHASE #########################\n",
    "    ####################################################################\n",
    "    \n",
    "    for i in range(depth):        \n",
    "        # Upsampling\n",
    "        net = UpSampling2D(( 2,  2))(net)\n",
    "     \n",
    "\n",
    "        # Convolution\n",
    "        net = conv_relu(net, filters = features_per_convolution[depth - i - 1][-1][1], kernel_size = 2, strides = 1 , activation = 'relu', kernel_initializer = 'glorot_normal', activate_bn = True, bn_decay = 0.999, keep_prob=1.0)\n",
    "        \n",
    "        data_temp_size.append(data_temp_size[-1] * 2)\n",
    "\n",
    "        # concatenation (see U-net article)\n",
    "        net = Concatenate(axis = 3)([relu_results[depth-i-1],net])\n",
    "      \n",
    "\n",
    "        # Classic convolutions\n",
    "        for conv_number in range(number_of_convolutions_per_layer[depth - i - 1]):\n",
    "            \n",
    "            net = conv_relu(net, filters = features_per_convolution[depth - i - 1][conv_number][1], kernel_size = size_of_convolutions_per_layer[depth - i - 1][conv_number], strides = 1 , activation = 'relu', kernel_initializer = 'glorot_normal', activate_bn = True, bn_decay = 0.999, keep_prob=1.0)\n",
    "            \n",
    "            \n",
    "        data_temp = net\n",
    "\n",
    "   \n",
    "    net = Conv2D(filters = n_classes, kernel_size = 1, strides = 1, name = 'finalconv', padding = 'same',   activation = \"softmax\")(net)\n",
    "\n",
    "    model = Model(inputs = X, outputs = net)\n",
    "\n",
    "    \n",
    "\n",
    "    return model\n",
    "\n",
    "        \n",
    "    \n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred, smooth=1e-3):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return K.mean((2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth))\n",
    "\n",
    "def dice_myelin(y_true, y_pred, smooth=1e-3):\n",
    "    y_true_f = K.flatten(y_true[..., 1])\n",
    "    y_pred_f = K.flatten(y_pred[..., 1])\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return K.mean((2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth))\n",
    "\n",
    "def dice_axon(y_true, y_pred, smooth=1e-3):\n",
    "    y_true_f = K.flatten(y_true[..., 2])\n",
    "    y_pred_f = K.flatten(y_pred[..., 2])\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return K.mean((2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard for Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Name = \"TEM_sample_dataset-{}\".format(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime()))\n",
    "tensorboard = TensorBoard(log_dir = 'models/{}'.format(Name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TEM_sample_dataset-2019-06-09 01:00:19'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/vasha_local/test_ads/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    " model = uconv_net(training_config,  bn_updated_decay = None, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = keras.optimizers.Adam(lr=training_config['learning_rate'], beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 512, 512, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 512, 512, 16) 1216        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 512, 512, 16) 64          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 512, 512, 16) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 512, 512, 16) 6416        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 512, 512, 16) 64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 512, 512, 16) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 256, 256, 16) 6416        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 256, 256, 16) 64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 256, 256, 32) 4640        batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 256, 256, 32) 128         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 256, 256, 32) 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 256, 256, 32) 9248        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 256, 256, 32) 128         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 256, 256, 32) 0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 128, 128, 32) 25632       dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 128, 128, 32) 128         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 128, 128, 64) 18496       batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 128, 128, 64) 256         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 128, 128, 64) 0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 128, 128, 64) 36928       dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 128, 128, 64) 256         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 128, 128, 64) 0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 64, 64, 64)   102464      dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 64, 64, 64)   256         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 64, 64, 128)  73856       batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 64, 64, 128)  512         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 64, 64, 128)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 64, 64, 128)  147584      dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 64, 64, 128)  512         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 64, 64, 128)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 32, 128)  409728      dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 32, 32, 128)  512         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 64, 64, 128)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 64, 64, 128)  65664       up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 64, 64, 128)  512         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 64, 64, 128)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 64, 64, 256)  0           dropout_8[0][0]                  \n",
      "                                                                 dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 64, 64, 128)  295040      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 64, 64, 128)  512         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 64, 64, 128)  0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 64, 64, 128)  147584      dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 64, 64, 128)  512         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 64, 64, 128)  0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 128, 128, 128 0           dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 128, 128, 64) 32832       up_sampling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 128, 128, 64) 256         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 128, 128, 64) 0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 128, 128, 128 0           dropout_6[0][0]                  \n",
      "                                                                 dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 128, 128, 64) 73792       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 128, 128, 64) 256         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 128, 128, 64) 0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 128, 128, 64) 36928       dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 128, 128, 64) 256         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 128, 128, 64) 0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, 256, 256, 64) 0           dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 256, 256, 32) 8224        up_sampling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 256, 256, 32) 128         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 256, 256, 32) 0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 256, 256, 64) 0           dropout_4[0][0]                  \n",
      "                                                                 dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 256, 256, 32) 18464       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 256, 256, 32) 128         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 256, 256, 32) 0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 256, 256, 32) 9248        dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 256, 256, 32) 128         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 256, 256, 32) 0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2D)  (None, 512, 512, 32) 0           dropout_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 512, 512, 16) 2064        up_sampling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 512, 512, 16) 64          conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 512, 512, 16) 0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 512, 512, 32) 0           dropout_2[0][0]                  \n",
      "                                                                 dropout_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 512, 512, 16) 12816       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 512, 512, 16) 64          conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 512, 512, 16) 0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 512, 512, 16) 6416        dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 512, 512, 16) 64          conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 512, 512, 16) 0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "finalconv (Conv2D)              (None, 512, 512, 3)  51          dropout_20[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 1,557,507\n",
      "Trainable params: 1,554,627\n",
      "Non-trainable params: 2,880\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=adam, loss=\"categorical_crossentropy\", metrics=[\"accuracy\", dice_axon, dice_myelin, dice_coef])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_prep import *\n",
    "from data_augmentation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '/home/vasha_local/axondeepseg/TEM_striatum/data/Train/'\n",
    "test_path = '/home/vasha_local/axondeepseg/TEM_striatum/data/Validation/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = training_config[\"n_classes\"]\n",
    "thresholds = training_config[\"thresholds\"]\n",
    "patch_size = training_config[\"trainingset_patchsize\"]\n",
    "\n",
    "batch_size = training_config[\"batch_size\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_aug = False # Boolean Value to indicate whether you want to use Data Augmentation or not\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(not data_aug):\n",
    "    data_gen_args = dict() \n",
    "else: \n",
    "    data_gen_args = dict(horizontal_flip = flipping()[1],\n",
    "                        vertical_flip = flipping()[0], \n",
    "                        rotation_range = random_rotation()[0], \n",
    "                        width_shift_range = shifting(patch_size, n_classes)[1],\n",
    "                        height_shift_range = shifting(patch_size, n_classes) [0]\n",
    "                        )\n",
    "\n",
    "#Data Augmentation Dictionary for Validation \n",
    "data_gen_args_valid = dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sanity Check of augmented data dictionary\n",
    "data_gen_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_gen = Generator(batch_size,'/home/vasha_local/axondeepseg/TEM_striatum/data/Train/','images','masks',data_gen_args,save_to_dir = '/home/vasha_local/axondeepseg/TEM_striatum/data/Train/aug')\n",
    "valid_gen = Generator(batch_size, '/home/vasha_local/axondeepseg/TEM_striatum/data/Validation/','images','masks', data_gen_args_valid,save_to_dir = '/home/vasha_local/axondeepseg/TEM_striatum/data/Validation/aug')                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_steps = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/vasha_local/test_ads/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/vasha_local/test_ads/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/600\n",
      "Found 16 images belonging to 1 classes.\n",
      "Found 36 images belonging to 1 classes.\n",
      "Found 16 images belonging to 1 classes.\n",
      "Found 36 images belonging to 1 classes.\n",
      "4/4 [==============================] - 8s 2s/step - loss: 0.0051 - acc: 0.4740 - dice_axon: 1.3414e-04 - dice_myelin: 3.7722e-04 - dice_coef: 0.0029 - val_loss: 0.0549 - val_acc: 0.1043 - val_dice_axon: 7.7189e-04 - val_dice_myelin: 7.9142e-04 - val_dice_coef: 8.1535e-04\n",
      "Epoch 2/600\n",
      "4/4 [==============================] - 3s 733ms/step - loss: 0.0035 - acc: 0.8437 - dice_axon: 1.6759e-04 - dice_myelin: 3.9080e-04 - dice_coef: 0.0036 - val_loss: 0.0098 - val_acc: 0.6867 - val_dice_axon: 4.3079e-04 - val_dice_myelin: 0.0017 - val_dice_coef: 0.0033\n",
      "Epoch 3/600\n",
      "4/4 [==============================] - 2s 461ms/step - loss: 0.0027 - acc: 0.9366 - dice_axon: 2.5494e-04 - dice_myelin: 5.1325e-04 - dice_coef: 0.0042 - val_loss: 0.0048 - val_acc: 0.8537 - val_dice_axon: 8.3310e-04 - val_dice_myelin: 0.0015 - val_dice_coef: 0.0042\n",
      "Epoch 4/600\n",
      "4/4 [==============================] - 3s 835ms/step - loss: 0.0022 - acc: 0.9508 - dice_axon: 3.5204e-04 - dice_myelin: 5.7687e-04 - dice_coef: 0.0049 - val_loss: 0.0027 - val_acc: 0.8359 - val_dice_axon: 4.9825e-04 - val_dice_myelin: 0.0011 - val_dice_coef: 0.0047\n",
      "Epoch 5/600\n",
      "4/4 [==============================] - 4s 911ms/step - loss: 0.0017 - acc: 0.9701 - dice_axon: 3.1688e-04 - dice_myelin: 5.3490e-04 - dice_coef: 0.0055 - val_loss: 0.0031 - val_acc: 0.8619 - val_dice_axon: 8.6245e-04 - val_dice_myelin: 0.0020 - val_dice_coef: 0.0052\n",
      "Epoch 6/600\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.0013 - acc: 0.9643 - dice_axon: 3.4190e-04 - dice_myelin: 6.7131e-04 - dice_coef: 0.0059 - val_loss: 0.0022 - val_acc: 0.8818 - val_dice_axon: 0.0010 - val_dice_myelin: 0.0025 - val_dice_coef: 0.0057\n",
      "Epoch 7/600\n",
      "4/4 [==============================] - 4s 990ms/step - loss: 9.9614e-04 - acc: 0.9559 - dice_axon: 6.4808e-04 - dice_myelin: 0.0013 - dice_coef: 0.0064 - val_loss: 0.0015 - val_acc: 0.8762 - val_dice_axon: 9.1683e-04 - val_dice_myelin: 0.0029 - val_dice_coef: 0.0061\n",
      "Epoch 8/600\n",
      "4/4 [==============================] - 4s 925ms/step - loss: 7.2871e-04 - acc: 0.9655 - dice_axon: 6.9119e-04 - dice_myelin: 0.0014 - dice_coef: 0.0068 - val_loss: 0.0019 - val_acc: 0.8549 - val_dice_axon: 9.9558e-04 - val_dice_myelin: 0.0031 - val_dice_coef: 0.0063\n",
      "Epoch 9/600\n",
      "4/4 [==============================] - 4s 927ms/step - loss: 5.8485e-04 - acc: 0.9652 - dice_axon: 0.0011 - dice_myelin: 0.0019 - dice_coef: 0.0071 - val_loss: 0.0017 - val_acc: 0.8570 - val_dice_axon: 0.0013 - val_dice_myelin: 0.0033 - val_dice_coef: 0.0064\n",
      "Epoch 10/600\n",
      "4/4 [==============================] - 4s 875ms/step - loss: 4.6593e-04 - acc: 0.9712 - dice_axon: 0.0014 - dice_myelin: 0.0021 - dice_coef: 0.0072 - val_loss: 0.0014 - val_acc: 0.8861 - val_dice_axon: 0.0023 - val_dice_myelin: 0.0039 - val_dice_coef: 0.0066\n",
      "Epoch 11/600\n",
      "4/4 [==============================] - 4s 1s/step - loss: 4.4131e-04 - acc: 0.9665 - dice_axon: 0.0020 - dice_myelin: 0.0027 - dice_coef: 0.0073 - val_loss: 0.0017 - val_acc: 0.8490 - val_dice_axon: 0.0018 - val_dice_myelin: 0.0038 - val_dice_coef: 0.0065\n",
      "Epoch 12/600\n",
      "4/4 [==============================] - 4s 983ms/step - loss: 3.3904e-04 - acc: 0.9750 - dice_axon: 0.0021 - dice_myelin: 0.0026 - dice_coef: 0.0074 - val_loss: 0.0011 - val_acc: 0.8917 - val_dice_axon: 0.0030 - val_dice_myelin: 0.0048 - val_dice_coef: 0.0068\n",
      "Epoch 13/600\n",
      "4/4 [==============================] - 4s 915ms/step - loss: 4.5311e-04 - acc: 0.9595 - dice_axon: 0.0029 - dice_myelin: 0.0033 - dice_coef: 0.0073 - val_loss: 0.0030 - val_acc: 0.8689 - val_dice_axon: 0.0021 - val_dice_myelin: 0.0040 - val_dice_coef: 0.0066\n",
      "Epoch 14/600\n",
      "4/4 [==============================] - 4s 958ms/step - loss: 3.9191e-04 - acc: 0.9716 - dice_axon: 0.0018 - dice_myelin: 0.0021 - dice_coef: 0.0074 - val_loss: 0.0022 - val_acc: 0.8378 - val_dice_axon: 0.0012 - val_dice_myelin: 0.0030 - val_dice_coef: 0.0065\n",
      "Epoch 15/600\n",
      "4/4 [==============================] - 4s 907ms/step - loss: 5.1566e-04 - acc: 0.9591 - dice_axon: 0.0020 - dice_myelin: 0.0022 - dice_coef: 0.0074 - val_loss: 0.0017 - val_acc: 0.8411 - val_dice_axon: 0.0019 - val_dice_myelin: 0.0039 - val_dice_coef: 0.0065\n",
      "Epoch 16/600\n",
      "4/4 [==============================] - 5s 1s/step - loss: 3.8410e-04 - acc: 0.9662 - dice_axon: 0.0025 - dice_myelin: 0.0032 - dice_coef: 0.0074 - val_loss: 0.0013 - val_acc: 0.8682 - val_dice_axon: 0.0026 - val_dice_myelin: 0.0046 - val_dice_coef: 0.0066\n",
      "Epoch 17/600\n",
      "4/4 [==============================] - 4s 966ms/step - loss: 2.8221e-04 - acc: 0.9774 - dice_axon: 0.0018 - dice_myelin: 0.0021 - dice_coef: 0.0075 - val_loss: 0.0016 - val_acc: 0.8533 - val_dice_axon: 0.0032 - val_dice_myelin: 0.0045 - val_dice_coef: 0.0067\n",
      "Epoch 18/600\n",
      "4/4 [==============================] - 4s 958ms/step - loss: 4.8736e-04 - acc: 0.9556 - dice_axon: 0.0035 - dice_myelin: 0.0039 - dice_coef: 0.0074 - val_loss: 0.0014 - val_acc: 0.8780 - val_dice_axon: 0.0030 - val_dice_myelin: 0.0047 - val_dice_coef: 0.0067\n",
      "Epoch 19/600\n",
      "4/4 [==============================] - 4s 948ms/step - loss: 3.1239e-04 - acc: 0.9740 - dice_axon: 0.0028 - dice_myelin: 0.0031 - dice_coef: 0.0075 - val_loss: 0.0017 - val_acc: 0.8592 - val_dice_axon: 0.0029 - val_dice_myelin: 0.0048 - val_dice_coef: 0.0067\n",
      "Epoch 20/600\n",
      "4/4 [==============================] - 3s 827ms/step - loss: 3.7346e-04 - acc: 0.9690 - dice_axon: 0.0029 - dice_myelin: 0.0030 - dice_coef: 0.0075 - val_loss: 0.0011 - val_acc: 0.8871 - val_dice_axon: 0.0030 - val_dice_myelin: 0.0048 - val_dice_coef: 0.0068\n",
      "Epoch 21/600\n",
      "4/4 [==============================] - 4s 1s/step - loss: 3.2063e-04 - acc: 0.9694 - dice_axon: 0.0033 - dice_myelin: 0.0037 - dice_coef: 0.0075 - val_loss: 0.0011 - val_acc: 0.8963 - val_dice_axon: 0.0032 - val_dice_myelin: 0.0048 - val_dice_coef: 0.0068\n",
      "Epoch 22/600\n",
      "4/4 [==============================] - 3s 864ms/step - loss: 2.9753e-04 - acc: 0.9727 - dice_axon: 0.0033 - dice_myelin: 0.0033 - dice_coef: 0.0075 - val_loss: 0.0011 - val_acc: 0.9070 - val_dice_axon: 0.0036 - val_dice_myelin: 0.0049 - val_dice_coef: 0.0069\n",
      "Epoch 23/600\n",
      "4/4 [==============================] - 4s 903ms/step - loss: 2.9257e-04 - acc: 0.9725 - dice_axon: 0.0037 - dice_myelin: 0.0034 - dice_coef: 0.0075 - val_loss: 0.0011 - val_acc: 0.9101 - val_dice_axon: 0.0033 - val_dice_myelin: 0.0049 - val_dice_coef: 0.0069\n",
      "Epoch 24/600\n",
      "4/4 [==============================] - 4s 886ms/step - loss: 3.1182e-04 - acc: 0.9710 - dice_axon: 0.0034 - dice_myelin: 0.0035 - dice_coef: 0.0075 - val_loss: 0.0012 - val_acc: 0.8773 - val_dice_axon: 0.0039 - val_dice_myelin: 0.0052 - val_dice_coef: 0.0068\n",
      "Epoch 25/600\n",
      "4/4 [==============================] - 3s 819ms/step - loss: 2.2252e-04 - acc: 0.9809 - dice_axon: 0.0036 - dice_myelin: 0.0036 - dice_coef: 0.0075 - val_loss: 9.7456e-04 - val_acc: 0.9070 - val_dice_axon: 0.0039 - val_dice_myelin: 0.0050 - val_dice_coef: 0.0069\n",
      "Epoch 26/600\n",
      "4/4 [==============================] - 4s 1s/step - loss: 2.8513e-04 - acc: 0.9729 - dice_axon: 0.0040 - dice_myelin: 0.0038 - dice_coef: 0.0075 - val_loss: 0.0010 - val_acc: 0.9037 - val_dice_axon: 0.0036 - val_dice_myelin: 0.0050 - val_dice_coef: 0.0069\n",
      "Epoch 27/600\n",
      "4/4 [==============================] - 3s 859ms/step - loss: 2.3726e-04 - acc: 0.9783 - dice_axon: 0.0043 - dice_myelin: 0.0040 - dice_coef: 0.0075 - val_loss: 0.0013 - val_acc: 0.8780 - val_dice_axon: 0.0039 - val_dice_myelin: 0.0052 - val_dice_coef: 0.0068\n",
      "Epoch 28/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 4s 964ms/step - loss: 2.4829e-04 - acc: 0.9762 - dice_axon: 0.0046 - dice_myelin: 0.0044 - dice_coef: 0.0075 - val_loss: 0.0014 - val_acc: 0.8753 - val_dice_axon: 0.0043 - val_dice_myelin: 0.0050 - val_dice_coef: 0.0068\n",
      "Epoch 29/600\n",
      "4/4 [==============================] - 4s 937ms/step - loss: 1.9251e-04 - acc: 0.9833 - dice_axon: 0.0037 - dice_myelin: 0.0034 - dice_coef: 0.0076 - val_loss: 0.0015 - val_acc: 0.8770 - val_dice_axon: 0.0048 - val_dice_myelin: 0.0050 - val_dice_coef: 0.0068\n",
      "Epoch 30/600\n",
      "4/4 [==============================] - 4s 944ms/step - loss: 2.3315e-04 - acc: 0.9775 - dice_axon: 0.0047 - dice_myelin: 0.0042 - dice_coef: 0.0076 - val_loss: 0.0010 - val_acc: 0.9024 - val_dice_axon: 0.0036 - val_dice_myelin: 0.0050 - val_dice_coef: 0.0069\n",
      "Epoch 31/600\n",
      "4/4 [==============================] - 5s 1s/step - loss: 2.3914e-04 - acc: 0.9777 - dice_axon: 0.0046 - dice_myelin: 0.0043 - dice_coef: 0.0076 - val_loss: 9.8764e-04 - val_acc: 0.9067 - val_dice_axon: 0.0037 - val_dice_myelin: 0.0053 - val_dice_coef: 0.0069\n",
      "Epoch 32/600\n",
      "4/4 [==============================] - 4s 898ms/step - loss: 2.1427e-04 - acc: 0.9795 - dice_axon: 0.0041 - dice_myelin: 0.0034 - dice_coef: 0.0076 - val_loss: 0.0017 - val_acc: 0.8661 - val_dice_axon: 0.0046 - val_dice_myelin: 0.0049 - val_dice_coef: 0.0068\n",
      "Epoch 33/600\n",
      "4/4 [==============================] - 4s 951ms/step - loss: 1.8504e-04 - acc: 0.9825 - dice_axon: 0.0049 - dice_myelin: 0.0043 - dice_coef: 0.0076 - val_loss: 9.7619e-04 - val_acc: 0.9126 - val_dice_axon: 0.0046 - val_dice_myelin: 0.0054 - val_dice_coef: 0.0070\n",
      "Epoch 34/600\n",
      "4/4 [==============================] - 4s 916ms/step - loss: 2.0049e-04 - acc: 0.9809 - dice_axon: 0.0047 - dice_myelin: 0.0042 - dice_coef: 0.0076 - val_loss: 9.4878e-04 - val_acc: 0.9162 - val_dice_axon: 0.0046 - val_dice_myelin: 0.0055 - val_dice_coef: 0.0070\n",
      "Epoch 35/600\n",
      "4/4 [==============================] - 4s 905ms/step - loss: 2.3561e-04 - acc: 0.9783 - dice_axon: 0.0044 - dice_myelin: 0.0037 - dice_coef: 0.0076 - val_loss: 0.0015 - val_acc: 0.8921 - val_dice_axon: 0.0033 - val_dice_myelin: 0.0052 - val_dice_coef: 0.0069\n",
      "Epoch 36/600\n",
      "4/4 [==============================] - 5s 1s/step - loss: 3.5316e-04 - acc: 0.9703 - dice_axon: 0.0049 - dice_myelin: 0.0040 - dice_coef: 0.0075 - val_loss: 0.0012 - val_acc: 0.8920 - val_dice_axon: 0.0051 - val_dice_myelin: 0.0054 - val_dice_coef: 0.0069\n",
      "Epoch 37/600\n",
      "4/4 [==============================] - 4s 960ms/step - loss: 2.3204e-04 - acc: 0.9788 - dice_axon: 0.0043 - dice_myelin: 0.0034 - dice_coef: 0.0076 - val_loss: 9.4490e-04 - val_acc: 0.9092 - val_dice_axon: 0.0043 - val_dice_myelin: 0.0055 - val_dice_coef: 0.0070\n",
      "Epoch 38/600\n",
      "4/4 [==============================] - 4s 945ms/step - loss: 3.6766e-04 - acc: 0.9658 - dice_axon: 0.0037 - dice_myelin: 0.0036 - dice_coef: 0.0075 - val_loss: 0.0012 - val_acc: 0.8966 - val_dice_axon: 0.0030 - val_dice_myelin: 0.0050 - val_dice_coef: 0.0068\n",
      "Epoch 39/600\n",
      "4/4 [==============================] - 4s 926ms/step - loss: 2.7405e-04 - acc: 0.9753 - dice_axon: 0.0037 - dice_myelin: 0.0038 - dice_coef: 0.0075 - val_loss: 0.0011 - val_acc: 0.8845 - val_dice_axon: 0.0044 - val_dice_myelin: 0.0053 - val_dice_coef: 0.0069\n",
      "Epoch 40/600\n",
      "4/4 [==============================] - 4s 906ms/step - loss: 2.3096e-04 - acc: 0.9783 - dice_axon: 0.0045 - dice_myelin: 0.0038 - dice_coef: 0.0076 - val_loss: 0.0010 - val_acc: 0.9008 - val_dice_axon: 0.0050 - val_dice_myelin: 0.0053 - val_dice_coef: 0.0070\n",
      "Epoch 41/600\n",
      "4/4 [==============================] - 4s 1s/step - loss: 2.4867e-04 - acc: 0.9771 - dice_axon: 0.0049 - dice_myelin: 0.0043 - dice_coef: 0.0076 - val_loss: 8.5436e-04 - val_acc: 0.9189 - val_dice_axon: 0.0039 - val_dice_myelin: 0.0052 - val_dice_coef: 0.0070\n",
      "Epoch 42/600\n",
      "4/4 [==============================] - 4s 901ms/step - loss: 2.7956e-04 - acc: 0.9722 - dice_axon: 0.0040 - dice_myelin: 0.0034 - dice_coef: 0.0075 - val_loss: 0.0010 - val_acc: 0.8975 - val_dice_axon: 0.0040 - val_dice_myelin: 0.0053 - val_dice_coef: 0.0069\n",
      "Epoch 43/600\n",
      "4/4 [==============================] - 4s 901ms/step - loss: 1.5277e-04 - acc: 0.9872 - dice_axon: 0.0041 - dice_myelin: 0.0032 - dice_coef: 0.0076 - val_loss: 9.6854e-04 - val_acc: 0.9051 - val_dice_axon: 0.0042 - val_dice_myelin: 0.0053 - val_dice_coef: 0.0070\n",
      "Epoch 44/600\n",
      "4/4 [==============================] - 4s 926ms/step - loss: 2.1432e-04 - acc: 0.9792 - dice_axon: 0.0044 - dice_myelin: 0.0037 - dice_coef: 0.0076 - val_loss: 9.0016e-04 - val_acc: 0.9159 - val_dice_axon: 0.0041 - val_dice_myelin: 0.0054 - val_dice_coef: 0.0070\n",
      "Epoch 45/600\n",
      "4/4 [==============================] - 4s 890ms/step - loss: 2.3962e-04 - acc: 0.9772 - dice_axon: 0.0053 - dice_myelin: 0.0047 - dice_coef: 0.0076 - val_loss: 0.0012 - val_acc: 0.8996 - val_dice_axon: 0.0045 - val_dice_myelin: 0.0051 - val_dice_coef: 0.0069\n",
      "Epoch 46/600\n",
      "4/4 [==============================] - 5s 1s/step - loss: 2.2834e-04 - acc: 0.9782 - dice_axon: 0.0047 - dice_myelin: 0.0040 - dice_coef: 0.0076 - val_loss: 9.5459e-04 - val_acc: 0.9065 - val_dice_axon: 0.0050 - val_dice_myelin: 0.0057 - val_dice_coef: 0.0070\n",
      "Epoch 47/600\n",
      "4/4 [==============================] - 4s 933ms/step - loss: 2.4626e-04 - acc: 0.9753 - dice_axon: 0.0052 - dice_myelin: 0.0043 - dice_coef: 0.0076 - val_loss: 0.0012 - val_acc: 0.9011 - val_dice_axon: 0.0043 - val_dice_myelin: 0.0054 - val_dice_coef: 0.0070\n",
      "Epoch 48/600\n",
      "4/4 [==============================] - 4s 916ms/step - loss: 1.2544e-04 - acc: 0.9895 - dice_axon: 0.0044 - dice_myelin: 0.0035 - dice_coef: 0.0077 - val_loss: 9.8021e-04 - val_acc: 0.9114 - val_dice_axon: 0.0040 - val_dice_myelin: 0.0055 - val_dice_coef: 0.0070\n",
      "Epoch 49/600\n",
      "4/4 [==============================] - 4s 936ms/step - loss: 2.9143e-04 - acc: 0.9736 - dice_axon: 0.0049 - dice_myelin: 0.0041 - dice_coef: 0.0075 - val_loss: 8.5629e-04 - val_acc: 0.9207 - val_dice_axon: 0.0043 - val_dice_myelin: 0.0053 - val_dice_coef: 0.0070\n",
      "Epoch 50/600\n",
      "4/4 [==============================] - 4s 885ms/step - loss: 2.9913e-04 - acc: 0.9702 - dice_axon: 0.0045 - dice_myelin: 0.0039 - dice_coef: 0.0075 - val_loss: 0.0014 - val_acc: 0.8719 - val_dice_axon: 0.0048 - val_dice_myelin: 0.0054 - val_dice_coef: 0.0068\n",
      "Epoch 51/600\n",
      "4/4 [==============================] - 4s 1s/step - loss: 2.0460e-04 - acc: 0.9809 - dice_axon: 0.0049 - dice_myelin: 0.0043 - dice_coef: 0.0076 - val_loss: 7.9070e-04 - val_acc: 0.9255 - val_dice_axon: 0.0046 - val_dice_myelin: 0.0056 - val_dice_coef: 0.0071\n",
      "Epoch 52/600\n",
      "4/4 [==============================] - 4s 967ms/step - loss: 2.2280e-04 - acc: 0.9795 - dice_axon: 0.0048 - dice_myelin: 0.0040 - dice_coef: 0.0076 - val_loss: 0.0012 - val_acc: 0.9088 - val_dice_axon: 0.0037 - val_dice_myelin: 0.0052 - val_dice_coef: 0.0070\n",
      "Epoch 53/600\n",
      "4/4 [==============================] - 4s 911ms/step - loss: 1.5299e-04 - acc: 0.9857 - dice_axon: 0.0049 - dice_myelin: 0.0037 - dice_coef: 0.0076 - val_loss: 0.0013 - val_acc: 0.8811 - val_dice_axon: 0.0049 - val_dice_myelin: 0.0055 - val_dice_coef: 0.0069\n",
      "Epoch 54/600\n",
      "4/4 [==============================] - 4s 904ms/step - loss: 2.0574e-04 - acc: 0.9799 - dice_axon: 0.0051 - dice_myelin: 0.0039 - dice_coef: 0.0076 - val_loss: 0.0010 - val_acc: 0.9116 - val_dice_axon: 0.0039 - val_dice_myelin: 0.0053 - val_dice_coef: 0.0070\n",
      "Epoch 55/600\n",
      "4/4 [==============================] - 3s 872ms/step - loss: 1.8729e-04 - acc: 0.9822 - dice_axon: 0.0052 - dice_myelin: 0.0044 - dice_coef: 0.0076 - val_loss: 8.9882e-04 - val_acc: 0.9103 - val_dice_axon: 0.0053 - val_dice_myelin: 0.0056 - val_dice_coef: 0.0070\n",
      "Epoch 56/600\n",
      "3/4 [=====================>........] - ETA: 0s - loss: 2.3366e-04 - acc: 0.9766 - dice_axon: 0.0058 - dice_myelin: 0.0049 - dice_coef: 0.0076"
     ]
    }
   ],
   "source": [
    "model.fit_generator(train_gen, validation_data = valid_gen, steps_per_epoch=train_steps, validation_steps = 2,\n",
    "                    epochs= 600,  callbacks = [tensorboard])\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'valid_gen' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-ddc72d4918bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m## Dataset for prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'valid_gen' is not defined"
     ]
    }
   ],
   "source": [
    "## Save the Weights\n",
    "model.save_weights(\"TEM_sample_Model.h5\")\n",
    "\n",
    "## Dataset for prediction\n",
    "x, y = valid_gen.__getitem__(1)\n",
    "result = model.predict(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Ground Truth\n",
    "plt.imshow(y[7,:,:,2], cmap = \"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(result[7,:,:,2], cmap = \"gray\") # Predicted Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = tf.Variable(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ToDo - Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\'# Modify the lines below to use your image\\npath_img = Path(\"./TEM_striatum/data/Testing\")\\nfile_img = \"image_819.png\"'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''''# Modify the lines below to use your image\n",
    "path_img = Path(\"./TEM_striatum/data/Testing\")\n",
    "file_img = \"image_819.png\"'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOF while scanning triple-quoted string literal (<ipython-input-29-ab792477a621>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-29-ab792477a621>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    prediction = axon_segmentation(path_img, file_img, path_model, config_network, acquired_resolution=0.01, resampled_resolutions=0.01, verbosity_level=3)\u001b[0m\n\u001b[0m                                                                                                                                                           \n^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOF while scanning triple-quoted string literal\n"
     ]
    }
   ],
   "source": [
    "'''# In case you want to test the segmentation with a pre-trained model created using this notebook,\n",
    "# uncomment the line below.\n",
    "path_model = Path(\"./TEM_striatum/model/TEM_3c_512_2018-11-10_21-32-36/\")\n",
    "\n",
    "# reset the tensorflow graph for new testing\n",
    "tf.reset_default_graph()\n",
    "prediction = axon_segmentation(path_img, file_img, path_model, config_network, acquired_resolution=0.01, resampled_resolutions=0.01, verbosity_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOF while scanning triple-quoted string literal (<ipython-input-30-da19f92788a1>, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-30-da19f92788a1>\"\u001b[0;36m, line \u001b[0;32m13\u001b[0m\n\u001b[0;31m    plt.show()\u001b[0m\n\u001b[0m              \n^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOF while scanning triple-quoted string literal\n"
     ]
    }
   ],
   "source": [
    "'''file_img_seg = 'AxonDeepSeg.png'  # axon+myelin segmentation\n",
    "\n",
    "img_seg = imageio.imread(path_img / file_img_seg)\n",
    "img = imageio.imread(path_img / file_img)\n",
    "# Note: The arguments of the two function calls above use the pathlib syntax for path concatenation.\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(13,10))\n",
    "ax1, ax2 = axes[0], axes[1]\n",
    "ax1.set_title('Original image')\n",
    "ax1.imshow(img, cmap='gray')\n",
    "ax2.set_title('Prediction with the trained model')\n",
    "ax2.imshow(img_seg,cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
