{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset creation notebook\n",
    "\n",
    "This notebook shows how to build a dataset for the training of a new model in AxonDeepSeg. It covers the following steps:\n",
    "\n",
    "* How to structure the raw data.\n",
    "* How to define the parameters of the patch extraction and divide the raw labelled dataset into patches.\n",
    "* How to generate the training dataset of patches by combining all raw data patches.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 0: IMPORTS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AxonDeepSeg.data_management import dataset_building\n",
    "import os, shutil\n",
    "from scipy.misc import imread, imsave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 1: GENERATE THE DATASET.\n",
    "\n",
    "### Suggested procedure for training/validation split of the dataset:\n",
    "\n",
    "* **Example use case:** we have 6 labelled samples in our dataset. To respect the split convention (between 10-30% of samples kept for validation), we can keep 5 samples for the training and the remaining one for the validation. \n",
    "\n",
    "---\n",
    "##### The folder structure *before* the training/validation split:\n",
    "\n",
    "* ***folder_of_your_raw_data***\n",
    "\n",
    "     * **sample1**\n",
    "          * *image.png*\n",
    "          * *mask.png*\n",
    "          * *pixel_size_in_micrometer.txt*\n",
    "     * **sample2**\n",
    "          * *image.png*\n",
    "          * *mask.png*\n",
    "          * *pixel_size_in_micrometer.txt*\n",
    "            \n",
    "            ...\n",
    "            \n",
    "     * **sample6**\n",
    "          * *image.png*\n",
    "          * *mask.png*\n",
    "          * *pixel_size_in_micrometer.txt*\n",
    "            \n",
    "---\n",
    "#### The folder structure *after* the training/validation split:\n",
    "\n",
    "* ***folder_of_your_raw_data***\n",
    "\n",
    "    * **Train**\n",
    "        * **sample1**\n",
    "            * *image.png*\n",
    "            * *mask.png*\n",
    "            * *pixel_size_in_micrometer.txt*\n",
    "        * **sample2**\n",
    "            * *image.png*\n",
    "            * *mask.png*\n",
    "            * *pixel_size_in_micrometer.txt*\n",
    "            \n",
    "            ...\n",
    "            \n",
    "        * **sample5**\n",
    "            * *image.png*\n",
    "            * *mask.png*\n",
    "            * *pixel_size_in_micrometer.txt*\n",
    "            \n",
    "    * **Validation**\n",
    "        * **sample6**\n",
    "            * *image.png*\n",
    "            * *mask.png*\n",
    "            * *pixel_size_in_micrometer.txt*\n",
    "---         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Define the parameters of the patch extraction.\n",
    "\n",
    "* **path_raw_data**: Path of the folder that contains the raw data. Each labelled sample of the dataset should be in a different subfolder. For each sample (and subfolder), the expected files are the following:\n",
    "    * *\"image.png\"*: The microscopy sample image (uint8 format).\n",
    "    * *\"mask.png\"*: The microscopy sample image (uint8 format).\n",
    "    * *\"pixel_size_in_micrometer.txt\"*: A one-line text file with the value of the pixel size of the sample. For instance, if the pixel size of the sample is 0.02um, the value in the text file should be **\"0.02\"**.\n",
    "    \n",
    "* **path_patched_data**: Path of the folder that will contain the raw data divided into patches. Each sample (i.e. subfolder) of the raw dataset will be divided into patches and saved in this folder. For instance, if a sample of the original data is divided into 10 patches, the corresponding folder in the **path_patched_dataset** will contain 10 image and mask patches, named **image_0.png** to **image_9.png** and **mask_0.png** to **mask_9.png**, respectively. \n",
    "\n",
    "* **patch_size**: The size of the patches in pixels. For instance, a patch size of **128** means that each generated patch will be 128x128 pixels.\n",
    "\n",
    "* **general_pixel_size**: The pixel size (i.e. resolution) of the generated patches in micrometers. The pixel size will be the same for all generated patches. If the selected pixel size is different from the native pixel sizes of the samples, downsampling or upsampling will be performed. Note that the pixel size should be chosen by taking into account the modality of the dataset and the patch size.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths for the training samples\n",
    "path_raw_data_train = '/Users/Dropbox/raw_data_SEM/Train'\n",
    "path_patched_data_train = '/Users/Dropbox/patched_data_SEM/Train'\n",
    "\n",
    "# Define the paths for the validation samples\n",
    "path_raw_data_validation = '/Users/Dropbox/raw_data_SEM/Validation'\n",
    "path_patched_data_validation = '/Users/Dropbox/patched_data_SEM/Validation'\n",
    "\n",
    "patch_size = 256\n",
    "general_pixel_size = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Divide the training/validation samples into patches.\n",
    "\n",
    "In the **path_patched_data** folder defined above, the original samples are going to be split into patches of same size. For instance, the sample 1 of the training set of the example use case above will be split into *n* patches and its corresponding subfolder in the **path_patched_data** folder will have the following structure:\n",
    "\n",
    "---\n",
    "* ***folder_of_your_patched_data***\n",
    "\n",
    "    * **Train**\n",
    "        * **sample1**\n",
    "            * *image_0.png*\n",
    "            * *mask_0.png*\n",
    "            * *image_1.png* \n",
    "            * *mask_1.png*\n",
    "            * *image_2.png*\n",
    "            * *mask_2.png*\n",
    "            \n",
    "            ...\n",
    "            \n",
    "            * *image_n.png* \n",
    "            * *mask_n.png*        \n",
    "---\n",
    "\n",
    "\n",
    "* Run the *raw_img_to_patches* function on both *Train* and *Validation* subfolders to split the data into patches. Note the input param. **thresh_indices** is a list of the threshold values to use in order to generate the classes of the training masks. The default value is [0, 0.2, 0.8], meaning that the mask labels (background=0, myelin=0.5, axon=1) will be split into our 3 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]/Users/rudinakaprata/Documents/Aldo/ads_feb/ads_may/lib/python2.7/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "100%|██████████| 7/7 [00:09<00:00,  1.30s/it]\n",
      "100%|██████████| 1/1 [00:04<00:00,  4.34s/it]\n"
     ]
    }
   ],
   "source": [
    "# Split the *Train* dataset into patches\n",
    "dataset_building.raw_img_to_patches(path_raw_data_train, path_patched_data_train, thresh_indices = [0, 0.2, 0.8], patch_size=patch_size, resampling_resolution=general_pixel_size)\n",
    "\n",
    "# Split the *Validation* dataset into patches\n",
    "dataset_building.raw_img_to_patches(path_raw_data_validation, path_patched_data_validation, thresh_indices = [0, 0.2, 0.8], patch_size=patch_size, resampling_resolution=general_pixel_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3. Regroup all the divided patches in the same training/validation folder.\n",
    "\n",
    "Finally, to build the dataset folder that is going to be used for the training, all patches obtained from the different samples are regrouped into the same folder and renamed. The final training and validation folders will have the following structure (*m* is the total number of training patches and *p* is the total number of validation patches):\n",
    "\n",
    "---\n",
    "* ***folder_of_your_final_patched_data***\n",
    "\n",
    "    * **Train**\n",
    "         * *image_0.png*\n",
    "         * *mask_0.png*\n",
    "         * *image_1.png* \n",
    "         * *mask_1.png*\n",
    "         * *image_2.png*\n",
    "         * *mask_2.png*\n",
    "            \n",
    "         ...\n",
    "           \n",
    "         * *image_m.png* \n",
    "         * *mask_m.png*   \n",
    "         \n",
    "    * **Validation**\n",
    "         * *image_0.png*\n",
    "         * *mask_0.png*\n",
    "         * *image_1.png* \n",
    "         * *mask_1.png*\n",
    "         * *image_2.png*\n",
    "         * *mask_2.png*\n",
    "            \n",
    "         ...\n",
    "           \n",
    "         * *image_p.png* \n",
    "         * *mask_p.png*         \n",
    "         \n",
    "---\n",
    "\n",
    "Note that we define a random seed in the input of the *patched_to_dataset* function in order to reproduce the exact same images each time we run the function. This is done to enable the generation of the same training and validation sets (for reproducibility). Also note that the **type_** input argument of the function can be set to **\"unique\"** or **\"mixed\"** to specify if the generated dataset comes from the same modality, or contains more than one modality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path of the final training dataset\n",
    "path_final_dataset_train = '/Users/Dropbox/final_patched_data_SEM/Train'\n",
    "\n",
    "# Path of the final validation dataset\n",
    "path_final_dataset_validation = '/Users/Dropbox/final_patched_data_SEM/Validation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:05<00:00,  1.36it/s]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.45s/it]\n"
     ]
    }
   ],
   "source": [
    "# Regroup all training patches\n",
    "dataset_building.patched_to_dataset(path_patched_data_train, path_final_dataset_train, type_='unique', random_seed=2017)\n",
    "\n",
    "# Regroup all validation patches\n",
    "dataset_building.patched_to_dataset(path_patched_data_validation, path_final_dataset_validation, type_='unique', random_seed=2017)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
