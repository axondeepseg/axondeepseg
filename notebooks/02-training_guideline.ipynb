{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a new model\n",
    "\n",
    "In this notebook we will learn how to train a new model for axon & myelin segmentation. It covers the following scenario:\n",
    "\n",
    "* Train a model from scratch by defining the parameters of the network\n",
    "* Make inference using the trained model\n",
    "\n",
    "**Important:** If you have access to a GPU card, we strongly recommend you use it. By default, AxonDeepSeg will only use the CPU. To install the GPU compatible version of AxonDeepSeg, refer to the documentation: https://axondeepseg.readthedocs.io/en/latest/documentation.html#gpu-compatible-installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util import Retry\n",
    "\n",
    "# Scientific package imports\n",
    "import imageio\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from skimage import io\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Utils import\n",
    "from shutil import copy\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "import cgi\n",
    "import tempfile\n",
    "\n",
    "# AxonDeepSeg imports\n",
    "try:\n",
    "    from AxonDeepSeg.ads_utils import download_data\n",
    "except ModuleNotFoundError:\n",
    "    # Change cwd to project main folder \n",
    "    os.chdir(\"..\")\n",
    "    try :\n",
    "        from AxonDeepSeg.ads_utils import download_data\n",
    "    except:\n",
    "        raise\n",
    "except:\n",
    "    raise\n",
    "# If no exceptions were raised import all folders        \n",
    "from AxonDeepSeg.config_tools import validate_config\n",
    "from AxonDeepSeg.train_network import train_model\n",
    "from AxonDeepSeg.apply_model import axon_segmentation\n",
    "import AxonDeepSeg.ads_utils as ads \n",
    "from config import axonmyelin_suffix\n",
    "\n",
    "# reset the tensorflow graph for new training\n",
    "tf.reset_default_graph()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Download example data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running this notebook, please make sure you have split, patched, and organized your data in the correct way by running the [guide_dataset_building.ipynb](guide_dataset_building.ipynb) tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Train a new model\n",
    "#### 1.1. Define the name and path of the training set\n",
    "\n",
    "Here we assume that the training data folder has already been created by following the guidelines detailed in [guide_dataset_building.ipynb](https://github.com/neuropoly/axondeepseg/blob/master/notebooks/guide_dataset_building.ipynb).\n",
    "\n",
    "The expected structure of the training data folder is the following:\n",
    "\n",
    "~~~\n",
    "data\n",
    " └── Train\n",
    "      └── image_0.png\n",
    "      └── mask_0.png\n",
    "      └── image_1.png\n",
    "      └── mask_1.png\n",
    "          ...\n",
    " └── Validation\n",
    "      └── image_0.png\n",
    "      └── mask_0.png\n",
    "      └── image_1.png\n",
    "      └── mask_1.png\n",
    "          ...\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_training = Path(\"./training\")  #  folder containing training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Define the U-Net architecture and hyper-parameters\n",
    "\n",
    "Here we defined the network and training parameters (i.e. hyperparameters). We use a lighter network than the ones used in the original [AxonDeepSeg article](https://www.nature.com/articles/s41598-018-22181-4), because they require large GPU memory (>12GB). The network below runs on an NVIDIA TitanX in ~2h. Note that the architecture below might not produce satisfactory results on your data so you likely need to play around with the architecture and hyperparameters.\n",
    "\n",
    "**Important:** The pixel size is not defined at the training step. During inference however, the parameter `-t {SEM,TEM,OM}` sets the resampling resolution to 0.1µm or 0.01µm depending on the model (i.e., implying the pixel size of the training data should be around 0.1µm for SEM and OM, and 0.01µm for TEM). This is definitely a limitation of the current version of AxonDeepSeg, which we are planning to solve at some point (for more info, see [Issue #152](https://github.com/neuropoly/axondeepseg/issues/152)). \n",
    "\n",
    "**Note about data augmentation:**\n",
    "For each type of data augmentation, the order needs to be specified if you decide to apply more than one transformation sequentially. For instance, setting `da-0-shifting-activate` to `True` means that the shifting is the first transformation that will be applied to the sample(s) during training. The default ranges of transformations are:\n",
    "- **Shifing**: Random horizontal and vertical shifting between 0 and 10% of the patch size, sampled from a uniform distribution.\n",
    "- **Rotation**: Random rotation, angle between 5 and 89 degrees, sampled from a uniform distribution.\n",
    "- **Rescaling**: Random rescaling of a randomly sampled factor between 1/1.2 and 1.2.\n",
    "- **Flipping**: Random fipping: vertical fipping or horizontal fipping.\n",
    "- **Elastic deformation**: Random elastic deformation with uniformly sampled deformation coefficient α=[1–8] and fixed standard deviation σ=4.\n",
    "\n",
    "You can find more information about the range of transformations applied to the patches for each data augmentation technique in the file [data_augmentation.py](https://github.com/neuropoly/axondeepseg/blob/master/AxonDeepSeg/data_management/data_augmentation.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of network configuration for TEM data (small network trainable on a Titan X GPU card)\n",
    "config = {\n",
    "    \n",
    "# General parameters:    \n",
    "  \"n_classes\": 3,  # Number of classes. For this application, the number of classes should be set to **3** (i.e. axon pixel, myelin pixel, or background pixel).\n",
    "  \"thresholds\": [0, 0.2, 0.8],  # Thresholds for the 3-class classification problem. Do not modify.  \n",
    "  \"trainingset_patchsize\": 512,  # Patch size of the training set in pixels (note that the patches have the same size in both dimensions).  \n",
    "  \"trainingset\": \"TEM\",  # Name of the training set.\n",
    "  \"batch_size\": 8,  # Batch size, i.e. the number of training patches used in one iteration of the training. Note that a larger batch size will take more memory.\n",
    "  \"epochs\":600,\n",
    "  \"checkpoint_period\": 5, # Number of epoch after which the model checkpoint is saved.\n",
    "  \"checkpoint\": None, # Checkpoint to use to resume training. Option: \"loss\", \"accuracy\" or None.\n",
    "\n",
    "# Network architecture parameters:     \n",
    "  \"depth\": 4,  # Depth of the network (i.e. number of blocks of the U-net).\n",
    "  \"convolution_per_layer\": [2, 2, 2, 2],  # Number of convolution layers used at each block.\n",
    "  \"size_of_convolutions_per_layer\": [[5, 5], [3, 3], [3, 3], [3, 3]],  # Kernel size of each convolution layer of the network.\n",
    "  \"features_per_convolution\": [[[1, 16], [16, 16]], [[16, 32], [32, 32]], [[32, 64], [64, 64]], [[64, 128], [128, 128]]],  # Number of features of each convolution layer.\n",
    "  \"downsampling\": \"convolution\",  # Type of downsampling to use in the downsampling layers of the network. Option \"maxpooling\" for standard max pooling layer or option \"convolution\" for learned convolutional downsampling.\n",
    "  \"dropout\": 0.75,  # Dropout to use for the training. Note: In TensorFlow, the keep probability is used instead. For instance, setting this param. to 0.75 means that 75% of the neurons of the network will be kept (i.e. dropout of 25%).\n",
    "     \n",
    "# Learning rate parameters:    \n",
    "  \"learning_rate\": 0.01,  # Learning rate to use in the training.  \n",
    "  \"learning_rate_decay_activate\": True,  # Set to \"True\" to use a decay on the learning rate.  \n",
    "  \"learning_rate_decay_period\": 24000,  # Period of the learning rate decay, expressed in number of images (samples) seen.\n",
    "  \"learning_rate_decay_type\": \"polynomial\",  # Type of decay to use. An exponential decay will be used by default unless this param. is set to \"polynomial\" (to use a polynomial decay).\n",
    "  \"learning_rate_decay_rate\": 0.99,  # Rate of the decay to use for the exponential decay. This only applies when the user does not set the decay type to \"polynomial\".\n",
    "    \n",
    "# Batch normalization parameters:     \n",
    "  \"batch_norm_activate\": True,  # Set to \"True\" to use batch normalization during the training.\n",
    "  \"batch_norm_decay_decay_activate\": True,  # Set to \"True\" to activate an exponential decay for the batch normalization step of the training.  \n",
    "  \"batch_norm_decay_starting_decay\": 0.7,  # The starting decay value for the batch normalization. \n",
    "  \"batch_norm_decay_ending_decay\": 0.9,  # The ending decay value for the batch normalization.\n",
    "  \"batch_norm_decay_decay_period\": 16000,  # Period of the batch normalization decay, expressed in number of images (samples) seen.\n",
    "        \n",
    "# Weighted cost parameters:    \n",
    "  \"weighted_cost-activate\": True,  # Set to \"True\" to use weights based on the class in the cost function for the training.\n",
    "  \"weighted_cost-balanced_activate\": True,  # Set to \"True\" to use weights in the cost function to correct class imbalance. \n",
    "  \"weighted_cost-balanced_weights\": [1.1, 1, 1.3],  # Values of the weights for the class imbalance. Typically, larger weights are assigned to classes with less pixels to add more penalty in the cost function when there is a misclassification. Order of the classes in the weights list: background, myelin, axon.\n",
    "  \"weighted_cost-boundaries_sigma\": 2,  # Set to \"True\" to add weights to the boundaries (e.g. penalize more when misclassification happens in the axon-myelin interface).\n",
    "  \"weighted_cost-boundaries_activate\": False,  # Value to control the distribution of the boundary weights (if activated). \n",
    "    \n",
    "# Data augmentation parameters:\n",
    "  \"da-type\": \"all\",  # Type of data augmentation procedure. Option \"all\" applies all selected data augmentation transformations sequentially, while option \"random\" only applies one of the selected transformations (randomly) to the sample(s). List of available data augmentation transformations: 'random_rotation', 'noise_addition', 'elastic', 'shifting', 'rescaling' and 'flipping'. \n",
    "  \"da-0-shifting-activate\": True, \n",
    "  \"da-1-rescaling-activate\": False,\n",
    "  \"da-2-random_rotation-activate\": False,  \n",
    "  \"da-3-elastic-activate\": True, \n",
    "  \"da-4-flipping-activate\": True, \n",
    "  \"da-6-reflection_border-activate\": False\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3. Define training path and save configuration parameters\n",
    "\n",
    "Here we define the path where the new model will be saved. It is useful to add date+time in path definition in case multiple training are launched (to avoid conflicts).\n",
    "\n",
    "The network configuration parameters defined at 1.2. are saved into a .json file in the model folder. This .json file keeps tract of the network and model parameters in a structured way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path to where the trained model will be saved\n",
    "dir_name = Path(config[\"trainingset\"] + '_' + time.strftime(\"%Y-%m-%d\") + '_' + time.strftime(\"%H-%M-%S\"))\n",
    "path_model = \"../models\" / dir_name\n",
    "\n",
    "print(\"This training session's model will be saved in the folder: \" + str(path_model.resolve().absolute()))\n",
    "\n",
    "# Create directory\n",
    "if not os.path.exists(path_model):\n",
    "    os.makedirs(path_model)\n",
    "\n",
    "# Define file name of network configuration\n",
    "file_config = 'config_network.json'\n",
    "\n",
    "# Load/Write config file (depending if it already exists or not)\n",
    "fname_config = os.path.join(path_model, file_config)\n",
    "if os.path.exists(fname_config):\n",
    "    with open(fname_config, 'r') as fd:\n",
    "        config_network = json.loads(fd.read())\n",
    "else:\n",
    "    with open(fname_config, 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    with open(fname_config, 'r') as fd:\n",
    "        config_network = json.loads(fd.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4. Launch the training procedure\n",
    "\n",
    "The training can be launched by calling the *'train_model'* function. After each epoch, the function will display the loss and accuracy of the model. The model checkpoints will be saved according to the \"checkpoint_period\" parameter in \"config\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the tensorflow graph for new testing\n",
    "tf.reset_default_graph()\n",
    "\n",
    "train_model(str(path_training), str(path_model), config)\n",
    "# Note: For multi-OS compatibility of this notebook, paths were defined as Path objects from the pathlib module.\n",
    "# They need to be converted into strings prior to be given as arguments to train_model(), as some old-style string\n",
    "# concatenation (e.g. \"+\") are still used in it.\n",
    "# In your own application, simply defining paths with correct syntax for your OS as strings instead of Path\n",
    "# objects would be sufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5. Monitor the training with Tensorboard\n",
    "\n",
    "[TensorBoard](https://www.tensorflow.org/guide/summaries_and_tensorboard) can be used to monitor the training procedure (loss and accuracy graphs, gradients, activations, identify bugs, etc.). To run TensorBoard, activate ADS virtual environment and run:\n",
    "```\n",
    "tensorboard --logdir PATH_MODEL --port 6006\n",
    "```\n",
    "where `PATH_MODEL` corresponds to this notebook's `path_model` variable (folder where model is being trained), and `port` is the port number where the TensorBoard local web server will be sent to (e.g., port 6006). Once the command is run, open a web browser with the address:\n",
    "```\n",
    "http://localhost:6006/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6. Resume training from checkpoint\n",
    "\n",
    "To resume training from a checkpoint, change the \"checkpoint\" parameter in \"config\" from None to \"loss\" or \"accuracy\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model = \"../models/Path_of_the_model\" # Path of the model where the checkpoint is saved\n",
    "\n",
    "train_model(str(path_training), str(path_model), config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Test the trained model\n",
    "#### 2.1. Set the path of the test image to be segmented with the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the lines below to use your image\n",
    "path_img = Path(\"../AxonDeepSeg/models/default_TEM_model/data_test\")\n",
    "file_img = \"image.png\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Launch the image segmentation\n",
    "\n",
    "The target resolution of the current version of the models are 0.1 for the **'default_SEM_model'** and **'OM_model' (model_seg_pns_bf)**, and 0.01 for the **'default_TEM_model'**. In this case, our test sample is a TEM brain sample of the mouse, so we set resampled_resolutions to 0.01.\n",
    "\n",
    "For your own trained model, use a resampled_resolutions corresponding to the general_pixel_size that was set in the 01-guide_dataset_building notebook in section 1.1. \"Define the parameters of the patch extraction\" when you created the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case you want to test the segmentation with a pre-trained default model, uncomment the line below.\n",
    "#path_model = Path(\"../AxonDeepSeg/models/default_TEM_model\")\n",
    "\n",
    "# reset the tensorflow graph for new testing\n",
    "tf.reset_default_graph()\n",
    "prediction = axon_segmentation(path_img, file_img, path_model, config_network, resampled_resolutions=0.01, verbosity_level=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. Display the resulted segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_img_seg = Path(file_img).stem + str(axonmyelin_suffix)  # axon+myelin segmentation\n",
    "\n",
    "img_seg = ads.imread(path_img / file_img_seg)\n",
    "img = ads.imread(path_img / file_img)\n",
    "# Note: The arguments of the two function calls above use the pathlib syntax for path concatenation.\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(13,10))\n",
    "ax1, ax2 = axes[0], axes[1]\n",
    "ax1.set_title('Original image')\n",
    "ax1.imshow(img, cmap='gray')\n",
    "ax2.set_title('Prediction with the trained model')\n",
    "ax2.imshow(img_seg,cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
